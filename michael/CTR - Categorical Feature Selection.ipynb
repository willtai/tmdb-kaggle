{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael_cmx/repos/kaggle-exploration/data/CTR/train_less.csv\n",
      "Counting lines...\n",
      "Generating sampling indices...\n",
      "Generating pandas dataframe...\n",
      "Saving to file...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# from sklearn.metrics import log_loss\n",
    "# train_file = \"../data/CTR/train.csv\"\n",
    "# train = pd.read_csv(train_file, nrows=1000000)\n",
    "\n",
    "# Note that the sampling steps take a while. At least a couple minutes\n",
    "\n",
    "import os\n",
    "\n",
    "root_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "train_file = os.path.join(root_path, \"data/CTR/train.csv\")\n",
    "train_less_file = os.path.join(root_path, \"data/CTR/train_less.csv\")\n",
    "    \n",
    "def create_sample_and_save(sample_size):\n",
    "    \n",
    "    print (\"Counting lines...\")\n",
    "    with open(train_file) as f:\n",
    "        file_length = sum(1 for line in f) - 1\n",
    "    \n",
    "    print (\"Generating sampling indices...\")\n",
    "    skip = sorted(random.sample(range(1,file_length+1),file_length-sample_size)) \n",
    "    \n",
    "    print (\"Generating pandas dataframe...\")\n",
    "    train = pd.read_csv(train_file, skiprows=skip)\n",
    "    \n",
    "    print (\"Saving to file...\")\n",
    "    train.to_csv(train_less_file, index=False)\n",
    "\n",
    "    \n",
    "sample_size = int(5*10**6)\n",
    "\n",
    "if not os.path.exists(train_less_file):\n",
    "    print (train_less_file)\n",
    "    create_sample_and_save(sample_size)\n",
    "\n",
    "train = pd.read_csv(train_less_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce it even further because I don't know why my memory is so damn low\n",
    "train = train[:int(10**6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>click</th>\n",
       "      <th>hour</th>\n",
       "      <th>C1</th>\n",
       "      <th>banner_pos</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_domain</th>\n",
       "      <th>site_category</th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_domain</th>\n",
       "      <th>...</th>\n",
       "      <th>device_type</th>\n",
       "      <th>device_conn_type</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>C16</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10001264480619467364</td>\n",
       "      <td>0</td>\n",
       "      <td>14102100</td>\n",
       "      <td>1002</td>\n",
       "      <td>0</td>\n",
       "      <td>84c7ba46</td>\n",
       "      <td>c4e18dd6</td>\n",
       "      <td>50e219e0</td>\n",
       "      <td>ecad2386</td>\n",
       "      <td>7801e8d9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21689</td>\n",
       "      <td>320</td>\n",
       "      <td>50</td>\n",
       "      <td>2496</td>\n",
       "      <td>3</td>\n",
       "      <td>167</td>\n",
       "      <td>100191</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10005541670676403131</td>\n",
       "      <td>0</td>\n",
       "      <td>14102100</td>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>e151e245</td>\n",
       "      <td>7e091613</td>\n",
       "      <td>f028772b</td>\n",
       "      <td>ecad2386</td>\n",
       "      <td>7801e8d9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20984</td>\n",
       "      <td>320</td>\n",
       "      <td>50</td>\n",
       "      <td>2371</td>\n",
       "      <td>0</td>\n",
       "      <td>551</td>\n",
       "      <td>100217</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10005951398749600249</td>\n",
       "      <td>0</td>\n",
       "      <td>14102100</td>\n",
       "      <td>1005</td>\n",
       "      <td>0</td>\n",
       "      <td>1fbe01fe</td>\n",
       "      <td>f3845767</td>\n",
       "      <td>28905ebd</td>\n",
       "      <td>ecad2386</td>\n",
       "      <td>7801e8d9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15706</td>\n",
       "      <td>320</td>\n",
       "      <td>50</td>\n",
       "      <td>1722</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>-1</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10006789981076459409</td>\n",
       "      <td>0</td>\n",
       "      <td>14102100</td>\n",
       "      <td>1005</td>\n",
       "      <td>0</td>\n",
       "      <td>030440fe</td>\n",
       "      <td>08ba7db9</td>\n",
       "      <td>76b2941d</td>\n",
       "      <td>ecad2386</td>\n",
       "      <td>7801e8d9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20596</td>\n",
       "      <td>320</td>\n",
       "      <td>50</td>\n",
       "      <td>2161</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>-1</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10009699694430474960</td>\n",
       "      <td>1</td>\n",
       "      <td>14102100</td>\n",
       "      <td>1005</td>\n",
       "      <td>0</td>\n",
       "      <td>4dd0a958</td>\n",
       "      <td>79cf0c8d</td>\n",
       "      <td>f028772b</td>\n",
       "      <td>ecad2386</td>\n",
       "      <td>7801e8d9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20366</td>\n",
       "      <td>320</td>\n",
       "      <td>50</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>-1</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  click      hour    C1  banner_pos   site_id  \\\n",
       "0  10001264480619467364      0  14102100  1002           0  84c7ba46   \n",
       "1  10005541670676403131      0  14102100  1005           1  e151e245   \n",
       "2  10005951398749600249      0  14102100  1005           0  1fbe01fe   \n",
       "3  10006789981076459409      0  14102100  1005           0  030440fe   \n",
       "4  10009699694430474960      1  14102100  1005           0  4dd0a958   \n",
       "\n",
       "  site_domain site_category    app_id app_domain  ... device_type  \\\n",
       "0    c4e18dd6      50e219e0  ecad2386   7801e8d9  ...           0   \n",
       "1    7e091613      f028772b  ecad2386   7801e8d9  ...           1   \n",
       "2    f3845767      28905ebd  ecad2386   7801e8d9  ...           1   \n",
       "3    08ba7db9      76b2941d  ecad2386   7801e8d9  ...           1   \n",
       "4    79cf0c8d      f028772b  ecad2386   7801e8d9  ...           1   \n",
       "\n",
       "  device_conn_type    C14  C15  C16   C17  C18  C19     C20  C21  \n",
       "0                0  21689  320   50  2496    3  167  100191   23  \n",
       "1                0  20984  320   50  2371    0  551  100217   46  \n",
       "2                0  15706  320   50  1722    0   35      -1   79  \n",
       "3                0  20596  320   50  2161    0   35      -1  157  \n",
       "4                0  20366  320   50  2333    0   39      -1  157  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(train)) < 0.8\n",
    "features = [3,4,5,6,7,8,9,10,11,13,14,15,16,17,18,19,20,21,22,23]\n",
    "X_train = train[msk].iloc[:,features]\n",
    "X_test = train[~msk].iloc[:,features]\n",
    "y_train = train[msk].iloc[:,1]\n",
    "y_test = train[~msk].iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4480827875044258\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_test.values,np.ones(len(y_test))*y_train.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "We're going to use two simple models for now: the default Logistic Regression and Random Forest models in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble as sk_ensemble\n",
    "random_forest = sk_ensemble.RandomForestClassifier(n_estimators=25, max_depth=10)\n",
    "\n",
    "import sklearn.linear_model as sk_linear_model\n",
    "logistic_regression = sk_linear_model.LogisticRegression(solver=\"liblinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "Really, the whole point of this notebook is to explore the various options provided by sklearn's preprocessing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Encoding to ordinal variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major classes of categorical data, nominal and ordinal.\n",
    "\n",
    "In any nominal categorical data attribute, there is no concept of ordering amongst the values of that attribute. e.g. movie genres, type of weather (sunny, rainy, cloudy), etc.\n",
    "\n",
    "Ordinal categorical attributes have some sense or notion of order amongst its values. e.g. shirt sizes, education level etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the blog:\n",
    "`Our first method is changing every categorical feature to an ordinal one. The order will be selected randomly (for example, like the order in the dataset or in an alphabetical order). This method does not make much sense because if we are encoding New York as 1, Tehran as 2 and New Jersey as 3, our algorithm will assume that Tehran is more similar to New York than New Jersey.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My thoughts:\n",
    "\n",
    "Based on the dataset values, it does seem like the categorical features tend to be more nominal than ordinal, so I can't disagree with this statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'click', 'hour', 'C1', 'banner_pos', 'site_id', 'site_domain',\n",
       "       'site_category', 'app_id', 'app_domain', 'app_category', 'device_id',\n",
       "       'device_ip', 'device_model', 'device_type', 'device_conn_type', 'C14',\n",
       "       'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ordinal = X_train.values\n",
    "X_test_ordinal = X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encode each feature with `sklearn.preprocessing.LabelEncoder` - `Encode labels with value between 0 and n_classes-1.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, "
     ]
    }
   ],
   "source": [
    "label_encodings = []\n",
    "for i in range(X_train_ordinal.shape[1]):\n",
    "    label_encoding = preprocessing.LabelEncoder()\n",
    "    # We fit the label_encoding with our column of the categorical feature\n",
    "    # we combine the ordinals so that we can \"see\" every feature otherwise labelencoder will be missing\n",
    "    # some features (this should not lead to data leakage)\n",
    "    combined_ordinal = np.concatenate([X_train_ordinal[:, i], X_test_ordinal[:, i]])\n",
    "    label_encoding.fit(combined_ordinal.astype(str)) \n",
    "    label_encodings.append(label_encoding)\n",
    "    \n",
    "    # Then we also want to convert the features to their encodings\n",
    "    X_train_ordinal[:, i] = label_encoding.transform(X_train_ordinal[:, i].astype(str))\n",
    "    X_test_ordinal[:, i] = label_encoding.transform(X_test_ordinal[:, i].astype(str))\n",
    "    \n",
    "    # So we know the progress\n",
    "    print (i/X_train_ordinal.shape[1], end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4401909774421925\n"
     ]
    }
   ],
   "source": [
    "logistic_regression.fit(X_train_ordinal, y_train)\n",
    "y_pred = logistic_regression.predict_proba(X_test_ordinal)\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4005714379710452\n"
     ]
    }
   ],
   "source": [
    "random_forest.fit(X_train_ordinal, y_train)\n",
    "y_pred = random_forest.predict_proba(X_test_ordinal)\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "combined_ordinal = np.concatenate([X_train_ordinal, X_test_ordinal])\n",
    "\n",
    "one_hot_encoder.fit(X_train_ordinal)\n",
    "X_train_one_hot = one_hot_encoder.transform(X_train_ordinal)\n",
    "X_test_one_hot = one_hot_encoder.transform(X_test_ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3867051622724999\n"
     ]
    }
   ],
   "source": [
    "logistic_regression.fit(X_train_one_hot,y_train)\n",
    "y_pred = logistic_regression.predict_proba(X_test_one_hot)\n",
    "print(log_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43581061217782285\n",
      "(799839, 123110)\n"
     ]
    }
   ],
   "source": [
    "random_forest.fit(X_train_one_hot,y_train)\n",
    "y_pred = random_forest.predict_proba(X_test_one_hot)\n",
    "print(log_loss(y_test,y_pred))\n",
    "print(X_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding rare values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1002\n",
       "1         1005\n",
       "2         1005\n",
       "3         1005\n",
       "5         1005\n",
       "          ... \n",
       "999994    1005\n",
       "999995    1005\n",
       "999997    1005\n",
       "999998    1005\n",
       "999999    1005\n",
       "Name: C1, Length: 799839, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"C1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"C1\"].value_counts()[X_train[\"C1\"]].values < 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X_train_rare = copy.copy(X_train)\n",
    "X_test_rare = copy.copy(X_test)\n",
    "X_train_rare[\"test\"] = 0\n",
    "X_test_rare[\"test\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X_train_rare = copy.copy(X_train)\n",
    "X_test_rare = copy.copy(X_test)\n",
    "X_train_rare[\"test\"]=0\n",
    "X_test_rare[\"test\"]=1\n",
    "temp_df = pd.concat([X_train_rare,X_test_rare],axis=0)\n",
    "names = list(X_train_rare.columns.values)\n",
    "temp_df = pd.concat([X_train_rare,X_test_rare],axis=0)\n",
    "for i in names:\n",
    "    temp_df.loc[temp_df[i].value_counts()[temp_df[i]].values < 20, i] = \"RARE_VALUE\"\n",
    "for i in range(temp_df.shape[1]):\n",
    "    temp_df.iloc[:,i]=temp_df.iloc[:,i].astype('str')\n",
    "X_train_rare = temp_df[temp_df[\"test\"]==\"0\"].iloc[:,:-1].values\n",
    "X_test_rare = temp_df[temp_df[\"test\"]==\"1\"].iloc[:,:-1].values\n",
    "\n",
    "label_encoders = []\n",
    "for i in range(X_train_rare.shape[1]):\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(temp_df.iloc[:,:-1].iloc[:, i])\n",
    "    label_encoders.append(label_encoder)\n",
    "    X_train_rare[:, i] = label_encoder.transform(X_train_rare[:, i])\n",
    "    X_test_rare[:, i] = label_encoder.transform(X_test_rare[:, i])\n",
    "    \n",
    "one_hot_encoder.fit(X_train_rare)\n",
    "X_train_rare = one_hot_encoder.transform(X_train_rare)\n",
    "X_test_rare = one_hot_encoder.transform(X_test_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38783155508577577\n",
      "0.4230776412195197\n",
      "(799839, 5264)\n"
     ]
    }
   ],
   "source": [
    "logistic_regression.fit(X_train_rare,y_train)\n",
    "y_pred = logistic_regression.predict_proba(X_test_rare)\n",
    "print(log_loss(y_test,y_pred))\n",
    "random_forest.fit(X_train_rare,y_train)\n",
    "\n",
    "y_pred = random_forest.predict_proba(X_test_rare)\n",
    "print(log_loss(y_test,y_pred))\n",
    "print(X_train_rare.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
